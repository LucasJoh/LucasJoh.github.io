<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AutoHDR | Lucas Schmitt </title> <meta name="author" content="Lucas Schmitt"> <meta name="description" content="Personal site of Lucas Schmitt. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lucas-schmitt.de/AutoHDR/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lucas</span> Schmitt </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="nav-item active"> <a class="nav-link" href="/AutoHDR/">AutoHDR <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/HBmeta/">Handball Metastats </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">CV</a> <div class="dropdown-divider"></div> <a class="dropdown-item active" href="/AutoHDR/">AutoHDR</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/HBmeta/">handball metastats</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">AutoHDR</h1> <p class="post-description"></p> </header> <article> <p><em>joint work of <a href="https://stani-stein.com" rel="external nofollow noopener" target="_blank">Viktor Stein</a> and <a href="https://lucas-schmitt.de">Lucas Schmitt</a></em></p> <p>Best viewed in an HDR compatible browser (Chrome) on an HDR compatible display.</p> <h1 id="abstract">Abstract</h1> <p>This project develops an algorithm to optimize high dynamic range (HDR) image editing settings in Adobe Lightroom for RAW images. Current auto-adjustment algorithms do not fully utilize HDR’s expanded brightness spectrum, resulting in less dynamic images. Our solution employs a Vision Transformer (ViT) model trained on a small dataset of RAW images with corresponding Lightroom settings. The model predicts optimal adjustments for exposure, contrast, highlights, shadows, whites, blacks, vibrance, and saturation, enhancing HDR image quality. Key techniques include data augmentation and label smoothing to improve model performance. This algorithm offers photographers a tool for achieving superior HDR image enhancements with minimal manual adjustments.</p> <h1 id="introduction">Introduction</h1> <p>Modern photography software like Adobe Lightroom and Darktable play a crucial role in the digital photography workflow, particularly for photographers who shoot in RAW format. RAW files contain unprocessed data directly from a camera’s image sensor, preserving the highest possible quality and providing extensive flexibility for post-processing. Unlike JPEGs, which are compressed and processed in-camera, RAW files allow photographers to make significant adjustments to exposure, color balance, contrast, and other parameters without degrading image quality. This capability is essential for professional photographers and enthusiasts seeking to achieve the highest quality results.</p> <p>The workflow of shooting in RAW typically begins with capturing images using a camera set to save files in the RAW format. These files are then imported into software like Lightroom or Darktable, where photographers can adjust various settings to enhance the images. The software offers a wide range of tools for fine-tuning, such as adjusting white balance, exposure, shadows, highlights, and color saturation. This non-destructive editing process means that the original RAW file remains unchanged, and all adjustments are stored as metadata. This allows for endless experimentation and refinement until the desired outcome is achieved. The RAWs themself are usually captured as neutral as possible allowing the most flexibility in edit. This however also means that the RAWs are usually quite flat and grey, making the editing of every photo almost a necessity.</p> <p>Given the complexity and variety of adjustments available, finding the optimal settings can be a time-consuming process, especially if one edits a large set of images of an event. Therefore most photographers are deeply familiar with Lightroom’s Auto Settings (Shift + A). This algorithm suggests values for some of the most important settings (Exposure, Contrast, Highlights, Shadows, Whites, Blacks, Vibrance, Saturation and some more). Most of the time these suggestions yield vibrant pictures that only need small adjustments to a subset of these settings. Therefore, a usual workflow might be to apply autosettings to all images and to only retouch a subset of the settings for each image, saving a lot of time.</p> <h2 id="hdr-photography">HDR Photography</h2> <p>Since October 2023 Adobe Lightroom has added native support for high dynamic range (HDR) image editing. HDR images contain more data per pixel, allowing the image to reach higher brightness values without oversaturating shadows. An HDR compatible display will now be able to ramp up the brightness of these areas significantly, whilst still keeping the shadows dark.</p> <p>You can check if your current display supports HDR by comparing the images below. If they appear similar, then your display does not support HDR. On a proper HDR display the sun on the right picture should almost be blinding and shadows should be rich in detail, just as your eye would experience it in real life.</p> <table> <tr> <th>Unedited Raw</th> <th>Non HDR Image</th> <th>HDR Image</th> </tr> <tr> <td><img src="../assets/img/unedited.jpg" width="200"></td> <td><img src="../assets/img/non_hdr_expl.jpg" width="200"></td> <td><img src="../assets/img/hdr_expl.jpg" width="200"></td> </tr> </table> <p>Fig 1: The image on the left is an unedited RAW image, the one in the middle has been edited and exported using a standard non HDR workflow and the image on the right with an HDR workflow. If the two edited images appear the same to you, then your browser/display do not support HDR playback.</p> <p>HDR technology is still in its early stages, so most displays do not support it yet. However, your phone might, as it typically offers the best display quality for the average consumer. Most laptops can not increase the brightness of a subset of pixels significantly without also increasing the brightness of dark parts. Therefore the bright parts of the HDR image are artificially darkened, destroying the HDR effect.</p> <p>The only problem with Adobe’s HDR implementation is that the autosettings do not consider the expanded brightness space. They tend to compress the brightness scale down to the usual allowed brightness scale. Therefore the blinding sunset becomes just bright and the dark shadow becomes brighter. The whole image now seems as grey as if it were not using HDR. A photographer would now need to adjust every single setting to restore the HDR effect, negating the usefulness of the autosettings.</p> <table> <tr> <th>Adobe Autosettings</th> <th>Model Predicted Autosettings</th> </tr> <tr> <td><img src="../assets/img/AdobeHDR.jpg" width="250"></td> <td><img src="../assets/img/AutoHDR.jpg" width="250"></td> </tr> </table> <p>Fig 2: On the left the settings suggested by Lightroom, on the right the settings suggested by our algorithm. Notice how Lightroom’s implementation boosts the shadows and is not using the entire brightness spectrum available. We again point out the necessity for an HDR compatible browser/display.</p> <p>The aim of the project is to write an algorithm that, given a small training dataset of RAWs with the corresponding Lightroom settings, finds a good suggestion for the settings to properly make use of the HDR colorspace.</p> <h1 id="architectural-concerns">Architectural Concerns</h1> <p>Our model has 8 settings to play with. <em>Exposure</em> adjusts overall brightness, ensuring a balanced level where both shadows and highlights retain detail without overexposure or underexposure. <em>Contrast</em> controls the difference between dark and light areas, essential for HDR. <em>Highlights</em> manage brightness in lighter parts, crucial for avoiding overexposure and maintaining detail in bright regions. <em>Shadows</em> adjust brightness in darker areas, vital for revealing details without making them unnaturally bright. Similarly one can adjust <em>Whites</em>, <em>Blacks</em>, <em>Vibrance</em> and <em>Shadows</em>. That is why our model needs to understand the effect of the settings on both the darkest and the brightest areas of the images at the same time. In other words, we have long range dependencies. Our choice therefore lands on the Vision Transformer introduced in [Dosovitskiy et al., 2020].</p> <p>The settings are all in the interval (-100,100) except for Exposure which lies in (-5,5). So, we scale all these intervals down to (-1,1) and train our model to $(-1,1)^n$ by choosing $\operatorname{tanh}()$ as the final activation of our ViT. After training we rescale the logits to use them in Lightroom. To use the standard Google ViT, we replace the final layer as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">ViTForImageClassification</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div></div> <h1 id="loading-and-preprocessing-data">Loading and preprocessing Data</h1> <p>One of the main challenges is to load and preprocess the available data in an efficient way. Since we are using a pretrained Vision Transformer, we need to ensure that our patch size is consistent with the 14 x 14 patches used in [Dosovitskiy et al., 2020]. The easiest way to achieve this is to directly downsample and normalize to rgb images of size 224x224, as this is consistent with the downsampling of ImageNet employed in the foundation model. With the following workflow we preprocess RAW data to PyTorch-tensors containing the normalized image data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">rgb_array</span><span class="p">):</span>
    <span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span> 
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
    <span class="p">])</span>
    <span class="n">img_tensor</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">rgb_array</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img_tensor</span>
</code></pre></div></div> <p>To get the corresponding labels we need to read the XMP-files and extract the values. As mentioned above it is also necessary to rescale all values to $(-1,1)$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">values</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">5</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span><span class="nf">float</span><span class="p">(</span><span class="n">root</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">.//rdf:Description[@crs:Exposure2012]</span><span class="sh">'</span><span class="p">,</span> <span class="n">ns</span><span class="p">).</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">{http://ns.adobe.com/camera-raw-settings/1.0/}Exposure2012</span><span class="sh">'</span><span class="p">]),</span>
    <span class="mi">100</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span><span class="nf">float</span><span class="p">(</span><span class="n">root</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">.//rdf:Description[@crs:Contrast2012]</span><span class="sh">'</span><span class="p">,</span> <span class="n">ns</span><span class="p">).</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">{http://ns.adobe.com/camera-raw-settings/1.0/}Contrast2012</span><span class="sh">'</span><span class="p">]),</span>
    <span class="bp">...</span>
<span class="p">]</span>
</code></pre></div></div> <p>Throughout the process of development it turned out that loading a RAW using rawpy is the most time expensive task in the data-preparation process. Nevertheless, we want to stick to the PyTorch-Dataset framework to make use of the Pytorch-Dataloader later on. As a consequence, we need a framework where training data can be directly accessed without reloading the RAWs every time.</p> <p>To solve this problem we separated the Dataset architecture into three parts: <em>RawImageDatatset</em>, <em>ImageDataset</em> and <em>AugmentedDataset</em>. The task distribution is now the following: The first one is used to access the RAW and XMP files and does all the preprocessing work, the second one uses a RawImageDataset to store all needed data in a way it can be accessed time efficiently. The last one offers all possibilities of data augmentation or label smoothing without interfering with the technical parts.</p> <p>To bring theses structures together, we initialize a RawImageDataset that enables us to access preprocessed data. We then hand this raw data to a ImageDataset which loads every image via the RawImageDataset framework and then stores it as PyTorch tensors. We are now able to directly access the tensors which are rapidly loaded using the torch.load function.</p> <p>Since we stick to the general framework, we are able to use methods from torch.utils.data that do further ML related preprocessing as splitting the dataset or creating batches for training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw_data</span> <span class="o">=</span> <span class="nc">RawImageDataset</span><span class="p">(</span><span class="n">directory_path</span><span class="p">)</span>
<span class="n">tensor_data</span> <span class="o">=</span> <span class="nc">ImageDataset</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">reload_data</span><span class="o">=</span><span class="n">reload_data</span><span class="p">)</span>
    
<span class="n">base_data</span><span class="p">,</span> <span class="n">val_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">random_split</span><span class="p">(</span><span class="n">tensor_data</span><span class="p">,</span> <span class="n">validation_split</span><span class="p">)</span>
</code></pre></div></div> <h1 id="model-training">Model training</h1> <p>Our training data is quite limited (~350 images). Thus, we followed two approaches from the beginning: utilizing a pretrained foundation model and data augmentation. In contrast to the recommendation of using high resolution images for downstream tasks [Dosovitskiy et al., 2020], we instead scale down to 244 x 244 to match the pretraining data. We did this initially for faster training during the prototyping phase, but noticed, that this resolution is sufficient for our task.</p> <p>As the labels are continuous values we employ an MSE loss and train using Adam in 10 epochs using batches of size 12 with a validation split of [0.8, 0.2] and a low learning rate of 0.0005.</p> <h2 id="with-and-without-pretraining">With and without pretraining</h2> <p>We initialize Google’s vit-base-patch16-224 ViT, replace the classifier and start training. We expected that during fine tuning we would need to carefully consider which layers to freeze and which layers to train. In actuality the naive approach of letting the model adjust all training parameters with the same learning rate works incredibly well converging after essentially one epoch. Therefore we also compared training without pretraining and see, that whilst convergence is a bit slower, the model also learns to capture the correct relationship.</p> <table> <tr> <th>With Pretraining</th> <th>Without Pretraining</th> </tr> <tr> <td><img src="../assets/img/pretrained_loss.png" width="350"></td> <td><img src="../assets/img/unpretrained_loss.png" width="350"></td> </tr> </table> <p>Fig 3: We see that the network pretty much converges after the first epoch until it eventually overfits. We will later try to mitigate the overfitting using label smoothing (see section <a href="#label-smoothing">Label Smoothing</a>). In both cases the final loss is usually around 0.02.</p> <p>Even though both the pre- and the unpretrained approach both prove very successful, we try to further push the effectiveness of our training. The idea is that a photographer might want to establish a certain style for a single photo shoot. If he now were to edit a small subset of these images in that style, the algorithm can quickly pick up on it and edit the rest. For this however we need to learn effectively on very small datasets. We therefore introduce data augmentation. It will prove similarly effective (see section <a href="#Evaluating-Data-Augmentations">Evaluating Data Augmentations</a>).</p> <h1 id="falsification-attempt">Falsification Attempt</h1> <p>Before we pursue data augmentation, we want to better understand the networks almost unreasonable performance. For this, we investigate the training data and the attention heads.</p> <h2 id="understanding-the-data">Understanding the Data</h2> <p>Our first suspicion for the unreasonable performance of our network is, that the data has a very simple structure. It might be possible, that settings such as Exposure or Saturation are essentially the same for all images in the training data. If this were the case, the network could always make a constant guess without being penalized significantly. We are therefore interested in the underlying statistics of the training labels.</p> <p><img src="../assets/img/data_statistics.png" width="700"></p> <p>Fig 4: Histogram of the labels in the training dataset</p> <p>We can clearly see, that some labels are actually quite simple. Saturation and Vibrance almost always have the same value. We expect that the network learns low weights and a bias reflecting the value for these settings.</p> <p><img src="../assets/img/nework_wab.png" width="700"></p> <p>Fig 5: In red the bias value for each setting and in blue the average connection strength to that node.</p> <p>We can see that this hypothesis was false. There is no clear pattern of a specific bias with low connections to it. Keep in mind that due to regularization the average magnitude of incoming connections is also essentially the same for all nodes. Furthermore the plot is anything but constant for different runs indicating that the network is actually responding to the image and not just making a fixed guess.</p> <p>Still, we suspect that a fixed guess might perform quite well. We therefore calculate the mean and standard deviation for each label and construct a simple guesser that picks its label suggestions from a normal distribution with the calculated variance and standard deviation. This guesser considers only the label space and does not take the input image into consideration.</p> <p><img src="../assets/img/mean_and_std.png" width="700"></p> <p>Fig 6: Mean and standard deviation of labels in training dataset.</p> <p>We evaluate this guesser on the validation set with an 80, 20 training - validation split and get a quite consistent loss of ~8%. This is definitely a very good performance considering the guesser did not look at the actual image. It is therefore fair to say that the underlying data is quite homogenous. Still, the random guesser is fortunately outperformed by our ViT model, which consistently achieves loss rates of around ~2%.</p> <h2 id="understanding-the-attention-heads">Understanding the Attention Heads</h2> <p>We now seek to understand the attention heads. The hope is that there is a certain structure here, indicating that the network is actually considering different aspects of the input image. As the settings affect the brightness spectrum of the image, we hypothesize that the network should pay attention to shadows, highlights and especially bright light sources (such as the sun).</p> <p>The ViT works on 16 x 16 tokens plus the cls token in 12 layers using 12 attention heads. For our visualization we highlight the patches that were most attended by each attention head for the cls token. We select a subset of layers and attention maps to make it a bit less convoluted.</p> <table> <tr> <th>Input</th> <th>Attention Maps</th> </tr> <tr> <td><img src="../assets/img/attention_input.jpg" width="300"></td> <td><img src="../assets/img/attention_map.png" width="500"></td> </tr> </table> <p>Fig 7: The left image was provided as input to the model, and a subset of attention heads was chosen for the right visualization. We selected every second attention head from every second transformer layer.</p> <p>Although interpreting attention maps should always be done with a grain of salt, one can tell that heads generally focus on specific brightness regions. This indicates that the network’s suggestion is actually based on the input data as it pays attention to similar areas in the images as a photographer would do when determining brightness settings.</p> <p>Overall, it is fair to say that even though the underlying data is not too complicated, that is at least not taking obvious escapes such as learning specific values independent of the input.</p> <h1 id="data-augmentation">Data Augmentation</h1> <p>Having only a limited amount of labeled data at hand, the generation of synthetic data is a natural approach to improve the sufficiency and diversity of training data. Without such augmention, the model risks overfitting to the training data. The basic idea of augmenting data for training is to introduce minor modifications to the data such that it remains close to the original but exhibits slight variations. For computer vision tasks this means to one changes small aspects of the image while keeping the main content recognizable, e.g. change the background when the task is to detect an object in the foreground. For object detection tasks there are extensive surveys available describing applicable data augmentation methods and providing a numerical analysis of their performance, see [Kumar et al., 2023] and [Yang et al., 2022]. However, our problem sets a different task to solve: we aim to recognize objects and their luminosity relative to the rest of the image. Due to the lack of specific performance data on available methods for this particular problem, we select seven promising basic data augmentation methods and apply them to the problem to evaluate their effectiveness.</p> <h2 id="data-augmentation-methods">Data Augmentation methods</h2> <p>We follow the taxonomy of basic data augmentation methods proposed by [Kumar et al., 2023]. For common methods, we use the available implementations provided by torchvision. The last two augmentation methods, not available within any ML framework, were manually implemented based on the respective papers. In the following, we introduce each method that is used in the training process and give a brief heuristic explanation how we think the method could benefit or harm the training.</p> <h3 id="geometric-image-manipulation">Geometric Image Manipulation</h3> <p><strong>Rotation and Flipping</strong></p> <p>As a first basic method to augment our training data we use flipping and rotating which preserves the structure and content of the picture, thus minimizing the risk of loosing important information. However, due to its simplicity, it is not able to generate diverse data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">hflip</span><span class="p">(</span><span class="n">original_img</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">original_img</span><span class="p">,</span> <span class="mf">90.0</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">rotate</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">hflip</span><span class="p">(</span><span class="n">original_img</span><span class="p">),</span> <span class="mf">90.0</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/rotations.png" width="700"></p> <p><strong>Shearing</strong></p> <p>By randomly shearing the picture, we -heuristically speaking- providing the model with different perspectives on the picture. Technically, we are changing the proportion of the objects and their spatial relations. This seems to be a good approach for our task as the luminosity of the picture should not depend on the specific shapes of the objects. However, one drawback is that shearing can generate black, and thus dark, regions on the border of the image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomPerspective</span><span class="p">(</span><span class="n">distortion_scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">original_img</span><span class="p">)</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/random_perspective.png" width="700"></p> <h3 id="non-geometric-image-manipulation">Non-Geometric Image Manipulation</h3> <p><strong>Random Cropping and Resize</strong></p> <p>Randomly cropping a patch from the original picture aims to create a different context for the objects included. We hope to to exclude uninteresting or even distracting elements on the image edges and focus on the main content in the center. Of course this is based on the assumption that we do not loose any crucial information by cropping. As before, the structure and colors of the main content remain untouched.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">original_img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">original_img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="nc">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))(</span><span class="n">original_img</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/random_cropping.png" width="700"></p> <p><strong>Distortion</strong></p> <p>Instead of loosing a whole region of the picture and leaving another region completely untouched, we try to add uncertainty to the structure on the whole picture. By adding distortion we reduce the sharpness of the edges of the objects. Since the task possibly involves detecting regions of varying light intensity, which are usually not separated by sharp edges, this approach hopefully supports the model training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ElasticTransform</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">[</span><span class="mf">50.</span><span class="o">+</span><span class="mf">50.</span><span class="o">*</span><span class="n">i</span><span class="p">])(</span><span class="n">original_img</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/distortion.png" width="700"></p> <p><strong>Gaussian blurring</strong></p> <p>With the same heuristics as before we apply a gaussian blur to the whole picture. As the object itself stays untouched in terms of shape and luminosity, this augmentation method should also go along well with our training task.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">transforms</span><span class="p">.</span><span class="nc">GaussianBlur</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">5.0</span><span class="p">))(</span><span class="n">original_img</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/gaussian_blur.png" width="700"></p> <h3 id="image-erasing">Image Erasing</h3> <p>By taking out parts of the image one hopefully drops out dominant regions that could prevent the model from learning less sensitive information beforehand. Without them, we enhance a more robust model. However, these methods may inadvertently remove important parts relevant to our task. Known examples for Image Erasing are random erasing, cutout or hide-and-seek, see [Kumar et al., 2023]. <strong>Gridmask deletion</strong> The perviously mentioned dropout methods have two main problems for our task. Since they delete a continuous region or an excessive amount of data they tend to delete important parts for our task, i.e. as our problem cannot be fully reduced to object identification we cannot be sure which part of the background is important. To overcome these problems, in [Chen et al., 2020] the so-called GridMask data augmentation method is introduced. Here a grid consisting of small mask units is created, where the parameter \(r\in (0,1)\) denotes the ratio of the shorter visible edge in a unit, and the unit size \(d=\text{random}(d_{min},d_{max})\) is randomly chosen. Lastly the distances \(\delta_x,\,\delta_y\in (0,d-1)\) between the first intact unit and the boundary of the image are also chosen randomly. For these parameters a grid mask is created which is later applied to the actual image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grid_mask</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">delta_x</span><span class="p">,</span> <span class="n">delta_y</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ones_l</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">r</span><span class="o">*</span><span class="n">d</span><span class="p">)</span>
    <span class="n">zeros_l</span> <span class="o">=</span> <span class="n">d</span><span class="o">-</span><span class="n">ones_l</span>
    <span class="n">start_x</span><span class="p">,</span> <span class="n">start_y</span> <span class="o">=</span> <span class="n">delta_x</span><span class="p">,</span> <span class="n">delta_y</span>

    <span class="k">while</span> <span class="n">start_x</span><span class="o">&lt;=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">end_x</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">start_x</span><span class="o">+</span><span class="n">zeros_l</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="k">while</span>  <span class="n">start_y</span><span class="o">&lt;=</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">end_y</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">start_y</span><span class="o">+</span><span class="n">zeros_l</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">mask</span><span class="p">[:,</span><span class="n">start_x</span><span class="p">:</span><span class="n">end_x</span><span class="p">,</span> <span class="n">start_y</span><span class="p">:</span><span class="n">end_y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">start_y</span> <span class="o">=</span> <span class="n">end_y</span> <span class="o">+</span> <span class="n">ones_l</span>
        <span class="n">start_x</span> <span class="o">=</span> <span class="n">end_x</span> <span class="o">+</span> <span class="n">ones_l</span>
        <span class="n">start_y</span> <span class="o">=</span> <span class="n">delta_y</span>   

    <span class="k">return</span> <span class="n">mask</span>
</code></pre></div></div> <p>The experiment results in [Chen et al., 2020] show a higher accuracy when training a ResNet under the usage of GridMask compared to standard image erasing methods on the ImageNet Dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">gridmask_deletion</span><span class="p">(</span><span class="n">original_img</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">d_min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">d_max</span><span class="o">=</span><span class="mi">70</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/grid_mask.png" width="700"></p> <h3 id="advanced-image-manipulation">Advanced Image Manipulation</h3> <p><strong>Local Rotation</strong> This method is introduced in [Kim et al., 2021] as part of a collection of local augmentation methods. In this case local rotation can be seen as a further development of global rotation. It was developed as an augmentation method for CNNs. As CNNs are biased to local features which is a disadvantage for generalization, we cut the picture in four patches that are then randomly rotated and glued together. In this way we might break up some strong local features, which should be advantageous for our problem that is mainly interested in the global luminosity. In [Kim et al., 2021] it is stated that the CIFAR100 test accuracy for a ResNet is superior if local rotation is used compared to global rotation.</p> <p>The local rotation introduces significant discontinuities into the image. This might be detrimental for tasks such as object recognition, as permutations might lead to objects being ripped apart. But for our task the locations should not destroy the training data, as the global illumination of the image stays essentially the same.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">original_img</span><span class="p">]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">local_rotation</span><span class="p">(</span><span class="n">original_img</span><span class="p">))</span>
<span class="nf">plot_images</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</code></pre></div></div> <p><img src="../assets/img/local_rotation.png" width="700"></p> <h1 id="label-smoothing">Label smoothing</h1> <p>Label smoothing tackles the problem that the labels in the dataset are noisy. This noise is especially relevant in our dataset, as in the artistic process of editing a photo, there are no right or wrong settings. Furthermore if you were to give a photographer the same photo to edit twice, we are quite certain, that the result would not be the same.</p> <p>[Szegedy et al., 2016] introduces label smoothing for classification tasks, by assuming that for a small $\varepsilon&gt;0$ the training set label is correct with only probability $1-\varepsilon$ and incorrect otherwise. We now strive to come up with a similar mechanism for regression tasks reflecting the lack of a correct choice in the task.</p> <h2 id="label-smoothing-methods">Label smoothing methods</h2> <p>As there are no discrete classes but continuous values we work with two different approaches to smooth the labels. In the first approach, given a sequence of training labels, we apply a moving average across the dataset for each label dimension. For the second approach, we add random gaussian noise to each value, based on the assumptions that whilst the label does not need the exact value as given in the training data, it should still be roughly in the same ballpark. The implementation details of these smoothing methods are provided below. We hope, that this smoothing decreases overfitting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">smoothing</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">moving_average</span><span class="sh">'</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">moving_average</span><span class="sh">'</span><span class="p">:</span>
        <span class="c1"># Apply moving average smoothing
</span>        <span class="k">if</span> <span class="n">window_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">smoothed_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">convolve</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">window_size</span><span class="p">)</span><span class="o">/</span><span class="n">window_size</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">smoothed_labels</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">gaussian</span><span class="sh">'</span><span class="p">:</span>
        <span class="c1"># Apply Gaussian smoothing
</span>        <span class="n">smoothed_labels</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">ndimage</span><span class="p">.</span><span class="nf">gaussian_filter1d</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Unsupported smoothing method</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">smoothed_labels</span>
</code></pre></div></div> <h1 id="evaluating-data-augmentations">Evaluating Data Augmentations</h1> <p>For evaluation we iterate over every possible augmentation method and select hyperparameters such that the amount of data is increased by factor eight. This value is chosen since it is the maximal factor using flipping and rotation and we want to obtain comparable results. Each augmentation method is combined with either no label smoothing, moving average or gaussian smoothing. Overall we obtain 21 possible combinations of label smoothing and data augmentation. For each of them the model is trained 30 times with a 0.05/0.95 training/validation split, simulating extreme data scarcity. We average the validation losses across all 30 models.</p> <p><img src="../assets/img/compare_aug_losses.png" width="700"></p> <p>Fig 8: Comparison average epoch validation losses of different augmentations. See <a href="https://www.stani-stein.com/AutoHDR/#evaluating-data-augmentations" rel="external nofollow noopener" target="_blank">stani-stein.com/AutoHDR</a> for an interactive version of the plot.</p> <p>What immediately catches the eye, is that data augmentation in principle has a positive impact on the model’s performance. Sobering, however is the impact of label smoothing. Without data augmentation it even seems to have a negative effect. At least on augmented data the models that are trained on smoothed data perform better than the ones with untreated labels. This suggests the assumption that having a certain amount of training data available is necessary for label smoothing to work. But this question is up to another evaluation since the observed effect is not pronounced.</p> <p>It is hard to say what augmentation works the best. It is however fair to say, that smoothing does seem to help with the validation error, possibly due to reduced overfitting. Furthermore we are inclined to say, that image erasing performs the worst. This may be because the deleted spots are registered as shadows, messing up the algorithms understanding of the scenes lighting. Both effects are however not strong and require further inquiry.</p> <h1 id="conclusion">Conclusion</h1> <p>Our algorithm manages to find a photographer’s editing style from an extremely small dataset (15-20 images) allowing a photographer to edit a subset of a photo shoot and let the algorithm decide the rest. The algorithm also learns to match editing styles, that are typical for HDR photography solving our original goal. This greatly reduces the time needed to edit photos, mitigating one of the less fun aspects of photography.</p> <h1 id="references">References</h1> <p>[Chen et al., 2020] Chen, Pengguang, et al. “Gridmask data augmentation.” arXiv preprint arXiv:2001.04086 (2020).</p> <p>[Kim et al., 2021] Kim, Youmin, AFM Shahab Uddin, and Sung-Ho Bae. “Local augment: Utilizing local bias property of convolutional neural networks for data augmentation.” IEEE Access 9 (2021).</p> <p>[Kumar et al., 2023] Kumar, Teerath, et al. “Image data augmentation approaches: A comprehensive survey and future directions.” arXiv preprint arXiv:2301.02830 (2023).</p> <p>[Szegedy et al., 2016] Szegedy, Christian, et al. “Rethinking the inception architecture for computer vision.” Proceedings of the IEEE conference on computer vision and pattern recognition. (2016).</p> <p>[Yang et al., 2022] Yang, Suorong, et al. “Image data augmentation for deep learning: A survey.” arXiv preprint arXiv:2204.08610 (2022).</p> <p>[Dosovitskiy et al., 2020] Dosovitskiy, Alexey et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” ArXiv abs/2010.11929 (2020): n. pag.</p> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lucas Schmitt. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-curriculum-vitae",title:"Curriculum Vitae",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-autohdr",title:"AutoHDR",description:"",section:"Navigation",handler:()=>{window.location.href="/AutoHDR/"}},{id:"nav-handball-metastats",title:"Handball Metastats",description:"Longterm developement of players driven by data based analysis",section:"Navigation",handler:()=>{window.location.href="/HBmeta/"}},{id:"dropdown-cv",title:"CV",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-autohdr",title:"AutoHDR",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-handball-metastats",title:"handball metastats",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-speaker-at-the-annual-meeting-of-the-dfg-priority-program-lt-a-href-quot-https-www-foundationsofdl-de-2fbc6f67-page-4-quot-gt-spp-2298-lt-a-gt-lt-em-gt-theoretical-foundations-of-deep-learning-lt-em-gt-november-03-05-2025",title:"Speaker at the annual meeting of the DFG Priority Program &lt;a href=&quot;https://www.foundationsofdl.de/?2fbc6f67_page=4#&quot;&gt;SPP 2298&lt;/a&gt; &lt;em&gt;Theoretical Foundations of Deep Learning&lt;/em&gt; (November 03-05, 2025)",description:"",section:"News"},{id:"news-participant-at-the-workshop-on-the-lt-a-href-quot-https-www-birs-ca-events-2025-5-day-workshops-25w5469-quot-gt-mathematics-of-of-transformers-lt-a-gt-at-desy-hamburg-september-26-2025",title:"Participant at the workshop on the &lt;a href=&quot;https://www.birs.ca/events/2025/5-day-workshops/25w5469&quot;&gt;Mathematics of of Transformers&lt;/a&gt; at DESY, Hamburg (September 26, 2025)",description:"",section:"News"},{id:"news-poster-presentation-at-the-conference-on-lt-a-href-quot-https-www-birs-ca-events-2025-5-day-workshops-25w5469-quot-gt-mathematics-of-machine-learning-2025-lt-a-gt-in-hamburg-september-22-25-2025",title:"Poster presentation at the conference on &lt;a href=&quot;https://www.birs.ca/events/2025/5-day-workshops/25w5469&quot;&gt;Mathematics of Machine Learning 2025&lt;/a&gt; in Hamburg (September 22-25, 2025)",description:"",section:"News"},{id:"news-speaker-at-the-birs-workshop-lt-a-href-quot-https-www-birs-ca-events-2025-5-day-workshops-25w5469-quot-gt-mathematical-analysis-of-adversarial-machine-learning-lt-a-gt-in-oaxaca-mexico-august-17-22-2025",title:"Speaker at the BIRS-workshop &lt;a href=&quot;https://www.birs.ca/events/2025/5-day-workshops/25w5469&quot;&gt;Mathematical Analysis of Adversarial Machine Learning&lt;/a&gt; in Oaxaca, Mexico (August 17-22, 2025)",description:"",section:"News"},{id:"news-participant-at-the-phd-course-lt-a-href-quot-https-malga-unige-it-education-schools-tfml-quot-gt-theoretical-foundations-of-machine-learning-lt-a-gt-in-genoa-italy-august-17-22-2025",title:"Participant at the PhD course &lt;a href=&quot;https://malga.unige.it/education/schools/tfml/&quot;&gt;Theoretical Foundations of Machine Learning&lt;/a&gt; in Genoa, Italy (August 17-22, 2025)",description:"",section:"News"},{id:"news-poster-presentation-at-the-summer-school-lt-a-href-quot-https-mmaa-lakecomoschool-org-quot-gt-mathematical-analysis-and-applications-lt-a-gt-in-como-italy-june-09-13-2025",title:"Poster presentation at the summer school &lt;a href=&quot;https://mmaa.lakecomoschool.org/&quot;&gt;Mathematical Analysis and Applications&lt;/a&gt; in Como, Italy (June 09-13, 2025)",description:"",section:"News"},{id:"news-participant-at-the-workshop-lt-a-href-quot-https-www-math-cit-tum-de-math-forschung-gruppen-data-science-events-understanding-generalization-in-deep-learning-quot-gt-understanding-generalization-in-deep-learning-lt-a-gt-in-raitenhaslach-february-18-21-2025",title:"Participant at the workshop &lt;a href=&quot;https://www.math.cit.tum.de/math/forschung/gruppen/data-science/events/understanding-generalization-in-deep-learning/&quot;&gt;Understanding Generalization in Deep Learning&lt;/a&gt; in Raitenhaslach (February 18-21, 2025)",description:"",section:"News"},{id:"news-participant-at-the-annual-meeting-of-the-dfg-priority-program-lt-a-href-quot-https-www-foundationsofdl-de-2fbc6f67-page-4-quot-gt-spp-2298-lt-a-gt-lt-em-gt-theoretical-foundations-of-deep-learning-lt-em-gt-november-11-14-2024",title:"Participant at the annual meeting of the DFG Priority Program &lt;a href=&quot;https://www.foundationsofdl.de/?2fbc6f67_page=4#&quot;&gt;SPP 2298&lt;/a&gt; &lt;em&gt;Theoretical Foundations of Deep Learning&lt;/em&gt; (November 11-14, 2024)",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%75%63%61%73.%73%63%68%6D%69%74%74@%75%6E%69-%77%75%65%72%7A%62%75%72%67.%64%65","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/LucasJoh","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/lucas-schmitt-74b3052a0","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>